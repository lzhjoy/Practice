{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch: 10/100, Loss: 1.8176\n",
      "Epoch: 20/100, Loss: 1.3194\n",
      "Epoch: 30/100, Loss: 1.2420\n",
      "Epoch: 40/100, Loss: 1.2302\n",
      "Epoch: 50/100, Loss: 1.2254\n",
      "Epoch: 60/100, Loss: 1.2225\n",
      "Epoch: 70/100, Loss: 1.2205\n",
      "Epoch: 80/100, Loss: 1.2190\n",
      "Epoch: 90/100, Loss: 1.2177\n",
      "Epoch: 100/100, Loss: 1.2166\n",
      "\n",
      "训练完成!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "word2vec 的 skip-gram 简单实现\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # 词嵌入层：用于获取中心词的向量\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        # 上下文嵌入层：用于获取上下文词（正负样本）的向量\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # 权重初始化\n",
    "        init_range = 0.5 / self.embedding_dim\n",
    "        self.u_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.v_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, center_word, pos_word, neg_words):\n",
    "        # 获取中心词的嵌入向量\n",
    "        u_embeds = self.u_embeddings(center_word)\n",
    "        # 获取正样本词的嵌入向量\n",
    "        v_embeds = self.v_embeddings(pos_word)\n",
    "        # 获取负样本词的嵌入向量\n",
    "        neg_v_embeds = self.v_embeddings(neg_words)\n",
    "        \n",
    "        return u_embeds, v_embeds, neg_v_embeds\n",
    "\n",
    "# 负采样损失函数\n",
    "def negative_sampling_loss(u_embeds, v_embeds, neg_v_embeds):\n",
    "    \"\"\"\n",
    "    u_embeds: (batch_size, embedding_size)\n",
    "    v_embeds: (batch_size, embdeding_size)\n",
    "    neg_v_embeds: (batch_size, num_neg_samples, embedding_size)\n",
    "    \"\"\"\n",
    "    # 计算正样本的点积 (batch_size, 1)\n",
    "    pos_score = torch.sum(torch.mul(u_embeds, v_embeds), dim=1)\n",
    "    # 计算负样本的点积 (batch_size, num_neg_samples)\n",
    "    neg_score = torch.sum(torch.mul(u_embeds.unsqueeze(1), neg_v_embeds), dim=2)\n",
    "    \n",
    "    # 将正样本的点积通过 sigmoid 转换为概率，并取 log\n",
    "    pos_loss = F.logsigmoid(pos_score).squeeze()\n",
    "    \n",
    "    # 将负样本的点积通过 sigmoid 转换为概率，并取 log，注意要取负号\n",
    "    neg_loss = torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "    \n",
    "    # 结合正负样本的损失并求平均\n",
    "    loss = - (pos_loss + neg_loss).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设的玩具数据\n",
    "    corpus = [\"hello world\", \"word2vec is fun\", \"pytorch is great\"]\n",
    "    vocab = sorted(list(set(\" \".join(corpus).split())))\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {i: word for i, word in enumerate(vocab)}\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 100\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.01\n",
    "    num_neg_samples = 5\n",
    "    # 修改上下文窗口大小\n",
    "    window_size = 2\n",
    "\n",
    "    # 准备训练数据 (中心词，上下文词，负样本词)\n",
    "    training_data = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, center_word in enumerate(words):\n",
    "            # 遍历上下文窗口内的词\n",
    "            for j in range(1, window_size + 1):\n",
    "                # 检查左侧上下文\n",
    "                if i - j >= 0:\n",
    "                    context_word = words[i - j]\n",
    "                    # 随机采样负样本\n",
    "                    neg_samples = [w.item() for w in torch.multinomial(torch.ones(vocab_size), num_neg_samples, replacement=True) if int_to_vocab[w.item()] != context_word]\n",
    "                    training_data.append((vocab_to_int[center_word], vocab_to_int[context_word], neg_samples))\n",
    "                # 检查右侧上下文\n",
    "                if i + j < len(words):\n",
    "                    context_word = words[i + j]\n",
    "                    neg_samples = [w.item() for w in torch.multinomial(torch.ones(vocab_size), num_neg_samples, replacement=True) if int_to_vocab[w.item()] != context_word]\n",
    "                    training_data.append((vocab_to_int[center_word], vocab_to_int[context_word], neg_samples))\n",
    "\n",
    "    # 初始化模型和优化器\n",
    "    # (模型定义和损失函数与之前相同，此处省略)\n",
    "    model = SkipGram(vocab_size, embedding_dim)\n",
    "    optimizer = optim.SparseAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练循环\n",
    "    print(\"开始训练...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for center_word_idx, pos_word_idx, neg_word_indices in training_data:\n",
    "            # 将数据转换为 tensor\n",
    "            center_word = torch.LongTensor([center_word_idx])\n",
    "            pos_word = torch.LongTensor([pos_word_idx])\n",
    "            neg_words = torch.LongTensor([neg_word_indices])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            u_embeds, v_embeds, neg_v_embeds = model(center_word, pos_word, neg_words)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = negative_sampling_loss(u_embeds, v_embeds, neg_v_embeds)\n",
    "            \n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {total_loss/len(training_data):.4f}\")\n",
    "\n",
    "    print(\"\\n训练完成!\")\n",
    "    # 训练完成后，可以获取词向量\n",
    "    word_vectors = model.u_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的深度学习模型训练过程，除了你提到的 **模型（model）**、**优化器（optimizer）**、**损失函数（loss）**、**超参数（hyperparameters）** 和 **数据（data）** 之外，通常还包含以下几个关键组件：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 学习率调度器（Learning Rate Scheduler）\n",
    "这个组件用于在训练过程中动态调整学习率。由于在训练的不同阶段，理想的学习率是不同的（例如，开始时需要较高的学习率快速收敛，后期则需要较低的学习率来微调），学习率调度器可以自动实现这种调整，防止模型在收敛后震荡或陷入局部最优。常见的调度器包括余弦退火（Cosine Annealing）、指数衰减（Exponential Decay）和分阶段衰减（Step Decay）。\n",
    "\n",
    "### 2. 评估指标（Metrics）\n",
    "损失函数是模型训练优化的目标，而评估指标则是我们用来衡量模型性能的工具。它能更直观地反映模型在特定任务上的表现。例如，在分类任务中，除了交叉熵损失，我们还会用**准确率（Accuracy）**、**精确率（Precision）**、**召回率（Recall）**和 **F1 分数**来评估模型。\n",
    "\n",
    "### 3. 训练循环（Training Loop）\n",
    "这是将所有组件整合在一起的“引擎”。训练循环负责迭代数据、执行前向传播、计算损失、执行反向传播、更新模型参数，以及在每个周期（epoch）或批次（batch）后进行评估和记录。一个好的训练循环设计可以确保训练的稳定性和效率。\n",
    "\n",
    "### 4. 日志与可视化（Logging and Visualization）\n",
    "在训练过程中，记录关键信息（如损失、评估指标、学习率等）并将其可视化是至关重要的。这有助于研究人员和工程师实时监控训练进度，诊断问题，并比较不同实验的结果。常见的工具包括 TensorBoard、Weights & Biases 和 Comet ML。\n",
    "\n",
    "### 5. 早停（Early Stopping）\n",
    "这是一个重要的正则化技术和训练策略。它通过监控模型在验证集上的性能，当性能在连续几个周期内不再提升时，提前停止训练。这不仅可以防止模型过拟合，还能节省大量的计算资源。\n",
    "\n",
    "---\n",
    "\n",
    "总而言之，一个完整的深度学习项目不仅仅是搭建一个模型，更是一个系统化的工程，需要将这些组件有机地结合起来，才能高效、稳定地训练出性能优异的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
